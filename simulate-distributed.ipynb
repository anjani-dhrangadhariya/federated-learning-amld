{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate FL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load python packages.\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import typing\n",
    "import pickle\n",
    "import inspect\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import islice\n",
    "from tqdm import tqdm, trange\n",
    "from os import path, getcwd, makedirs\n",
    "from tensorboardX import SummaryWriter\n",
    "# NLP dataset loading\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append `torch_federated_learning` package path.\n",
    "torch_package_path = \"Federated_Learning_Workshop_AMLD2021/src/torch_federated_learning/\"\n",
    "sys.path.insert(0, torch_package_path)\n",
    "# Import FL simulation objects from the above directory. \n",
    "from options import (\n",
    "    args_parser,\n",
    "    get_nb_args,\n",
    "    print_train_stats,\n",
    ")\n",
    "from utils import (\n",
    "    #get_dataset, \n",
    "    num_batches_per_epoch,\n",
    "    average_weights, \n",
    "    exp_details,\n",
    ")\n",
    "from update import (\n",
    "    test_inference,\n",
    "    ClientShard,\n",
    ")\n",
    "from models import (\n",
    "    get_model, \n",
    "    get_optimizer,\n",
    ")\n",
    "from sampling import ade_iid\n",
    "# Define path and directory to save trained model and graphs.\n",
    "save_path = \"./save/\"\n",
    "model_save_dir = \"objects\"\n",
    "# Load default arguments.\n",
    "args = get_nb_args()\n",
    "# Create directory to save trained model weight `.pkl` (pickle) files.\n",
    "makedirs(path.join(save_path, model_save_dir), exist_ok=True)\n",
    "# Define paths.\n",
    "path_project = path.abspath('./')\n",
    "logger = SummaryWriter('./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.csv: 29.5MB [02:49, 174kB/s]                                              \n"
     ]
    }
   ],
   "source": [
    "train_iter = AG_NEWS(split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[475, 21, 30, 5297]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['here', 'is', 'an', 'example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "         label_list.append(label_pipeline(_label))\n",
    "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "         text_list.append(processed_text)\n",
    "         offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test.csv: 1.86MB [00:02, 789kB/s]                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1782 batches | accuracy    0.689\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.849\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.879\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time:  8.96s | valid accuracy    0.885 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1782 batches | accuracy    0.899\n",
      "| epoch   2 |  1000/ 1782 batches | accuracy    0.901\n",
      "| epoch   2 |  1500/ 1782 batches | accuracy    0.901\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time:  8.96s | valid accuracy    0.896 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1782 batches | accuracy    0.916\n",
      "| epoch   3 |  1000/ 1782 batches | accuracy    0.915\n",
      "| epoch   3 |  1500/ 1782 batches | accuracy    0.912\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time:  9.24s | valid accuracy    0.899 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1782 batches | accuracy    0.924\n",
      "| epoch   4 |  1000/ 1782 batches | accuracy    0.923\n",
      "| epoch   4 |  1500/ 1782 batches | accuracy    0.921\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time:  9.27s | valid accuracy    0.905 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1782 batches | accuracy    0.930\n",
      "| epoch   5 |  1000/ 1782 batches | accuracy    0.929\n",
      "| epoch   5 |  1500/ 1782 batches | accuracy    0.928\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time:  9.10s | valid accuracy    0.904 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1782 batches | accuracy    0.943\n",
      "| epoch   6 |  1000/ 1782 batches | accuracy    0.942\n",
      "| epoch   6 |  1500/ 1782 batches | accuracy    0.944\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time:  9.13s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1782 batches | accuracy    0.947\n",
      "| epoch   7 |  1000/ 1782 batches | accuracy    0.942\n",
      "| epoch   7 |  1500/ 1782 batches | accuracy    0.943\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time:  9.12s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1782 batches | accuracy    0.947\n",
      "| epoch   8 |  1000/ 1782 batches | accuracy    0.944\n",
      "| epoch   8 |  1500/ 1782 batches | accuracy    0.946\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time:  8.99s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1782 batches | accuracy    0.946\n",
      "| epoch   9 |  1000/ 1782 batches | accuracy    0.945\n",
      "| epoch   9 |  1500/ 1782 batches | accuracy    0.945\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time:  9.01s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1782 batches | accuracy    0.946\n",
      "| epoch  10 |  1000/ 1782 batches | accuracy    0.945\n",
      "| epoch  10 |  1500/ 1782 batches | accuracy    0.947\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time:  9.00s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "      scheduler.step()\n",
    "    else:\n",
    "       total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.912\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Federated Learning through Distributed Learning <br>\n",
    "\n",
    "Training a model on data shards to emulate distributed environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here user groups is a dictionary containing keys for every member (here 50) \n",
    "# with subsequent values containing row IDs allocated to that client shard.  \n",
    "\n",
    "# Setting 50 clients.\n",
    "args.num_users = 50\n",
    "# Setting only 1 global round\n",
    "args.epochs = 1\n",
    "# Setting 10 epochs on client shard.\n",
    "args.local_ep = 10\n",
    "# Setting `frac` to use only one client shard\n",
    "args.frac = 0.02 # 50 clients x 0.02 sampling fraction => 1 client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model = 'nlp'\n",
    "args.task = 'nlp'\n",
    "args.dataset = 'agnews'\n",
    "args.iid = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(dataset='agnews', epochs=10, frac=0.2, gpu='', iid=1, kernel_num=9, kernel_sizes='3,4,5', local_bs=8, local_ep=10, lr=0.01, max_pool='True', model='nlp', momentum=0.5, norm='batch_norm', num_channels=3, num_classes=10, num_filters=32, num_users=10, optimizer='sgd', seed=1, stopping_rounds=10, task='nlp', test_frac=0.1, unequal=0, verbose=1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the global model\n",
    "global_model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ade_iid(dataset, num_users):\n",
    "    \"\"\"Sample I.I.D. client data from Ade_corpus dataset\n",
    "    Args:\n",
    "        dataset: Dataset to be used to simulate client datasets.\n",
    "        num_users: Number of clients in federated learning.\n",
    "    Returns:\n",
    "        dict of text index\n",
    "    \"\"\"\n",
    "    num_items = int(len(dataset)/num_users)\n",
    "    dict_users, all_idxs = {}, [i for i in range(len(dataset))]\n",
    "    for i in range(num_users):\n",
    "        dict_users[i] = set(np.random.choice(all_idxs,\n",
    "                                             num_items,\n",
    "                                             replace=False))\n",
    "        all_idxs = list(set(all_idxs) - dict_users[i])\n",
    "    return dict_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_(args, train_dataset, test_dataset, custom_sampling=None):\n",
    "    \"\"\" Returns train and test datasets and a user group which is a dict where\n",
    "    the keys are the user index and the values are the corresponding data for\n",
    "    each of those users.\n",
    "    \"\"\"\n",
    "    if args.task == 'nlp':\n",
    "        assert args.dataset == \"agnews\", \"Parsed dataset not implemented.\"\n",
    "        # sample training data amongst users\n",
    "        if args.iid:\n",
    "            # Sample IID user data from Ade_corpus\n",
    "            user_groups = ade_iid(train_dataset, args.num_users)\n",
    "        else:\n",
    "            # Sample Non-IID user data from Ade_corpus\n",
    "            if args.unequal:\n",
    "                # Chose unequal splits for every user\n",
    "                raise NotImplementedError()\n",
    "            else:\n",
    "                # Chose equal splits for every user\n",
    "                user_groups = ade_noniid(train_dataset, args.num_users)\n",
    "                \n",
    "    return train_dataset, test_dataset, user_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with user groups\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "train_dataset, test_dataset, user_groups = get_dataset_(args, train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to the device (cpu or gpu).\n",
    "global_model.to(device)\n",
    "# Set the model to train mode \n",
    "# (i.e. green flag for model weights to be updated in backward pass).\n",
    "global_model.train()\n",
    "# Instatiate a copy of the untrained model weights to be used for Federated Averaging as a base.  \n",
    "global_weights = global_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample dataset to create client data shards.\n",
    "idxs_users = np.random.choice(range(args.num_users), 1, replace=False)\n",
    "# Get our brave client's id.\n",
    "lonely_client_idx = idxs_users[0]\n",
    "# We also copy the untrained global model weights to be used to load local trained weights\n",
    "updated_local_model = global_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
